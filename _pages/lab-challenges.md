---
title: "Lab Challenges"
permalink: /lab-challenges/
layout: single
author_profile: true
---

## Data Analysis Lab Challenges

Here you'll find detailed documentation of data analysis challenges I've completed, showcasing my problem-solving approach and technical skills development.

---

## üï∑Ô∏è Challenge 1: Web Scraping & Data Collection

### Problem Statement
Extract structured hockey team statistics from a live website and transform the data into an analysis-ready format for further processing and visualization.

### Approach
1. **HTTP Request Handling**: Used Python's requests library to retrieve webpage content
2. **HTML Parsing**: Implemented BeautifulSoup to navigate and extract data from HTML tables
3. **Data Structuring**: Organized extracted data into Pandas DataFrame
4. **Data Export**: Saved structured data to CSV format for future analysis

### Tools Used
- **Python** with Requests library
- **BeautifulSoup** for HTML parsing
- **Pandas** for data manipulation
- **Google Colab** for development environment

### Key Lessons Learned
- Web scraping requires careful handling of website structure changes
- Data extraction from HTML tables can be automated for reproducible results
- Proper error handling is essential for robust web scraping scripts
- Structured data export enables seamless integration with analysis workflows

**[View Full Project Details ‚Üí](/projects/web-scraping-challenge/)**

---

## üé¨ Challenge 2: Netflix Data Wrangling

### Problem Statement
Clean and transform a messy Netflix dataset containing inconsistent formatting, missing values, and data quality issues into a reliable, analysis-ready dataset.

### Approach
1. **Data Assessment**: Comprehensive profiling to identify data quality issues
2. **Missing Value Treatment**: Strategic handling of incomplete data across multiple columns
3. **Data Standardization**: Normalization of text formats and correction of data types
4. **Feature Engineering**: Creation of new analytical features from existing data
5. **Quality Validation**: Systematic checking of data consistency and completeness

### Tools Used
- **Python** with Pandas library
- **NumPy** for numerical operations
- **Kaggle** for dataset hosting and code execution

### Key Lessons Learned
- Systematic data inspection is crucial before beginning any cleaning process
- Different missing value strategies are required for different data types
- Feature engineering can significantly enhance analytical capabilities
- Data validation ensures the reliability of downstream analysis

**[View Full Project Details ‚Üí](/projects/netflix-data-wrangling/)**

---

## üö¢ Challenge 3: Titanic Survival Analysis

### Problem Statement
Conduct comprehensive exploratory data analysis on the Titanic dataset to identify factors influencing passenger survival rates and uncover meaningful patterns in the data.

### Approach
1. **Initial Exploration**: Dataset profiling and structure understanding
2. **Univariate Analysis**: Individual variable distribution analysis
3. **Bivariate Analysis**: Relationship investigation between variable pairs
4. **Multivariate Analysis**: Complex pattern detection across multiple variables
5. **Statistical Testing**: Hypothesis validation and significance testing

### Tools Used
- **Python** with Pandas and NumPy
- **Matplotlib** and **Seaborn** for visualization
- **Statistical analysis** techniques
- **Jupyter Notebooks** for interactive analysis

### Key Lessons Learned
- Exploratory data analysis reveals hidden patterns not apparent in raw data
- Visualization is essential for understanding complex relationships
- Statistical testing validates observational findings
- Demographic and socio-economic factors significantly influence outcomes

**[View Full Project Details ‚Üí](/projects/titanic-data-analysis/)**

---

## üè® Challenge 4: Hotel Revenue Analytics

### Problem Statement
Develop an interactive business intelligence dashboard to track hotel performance metrics, enable data-driven decision making, and provide real-time insights into revenue optimization opportunities.

### Approach
1. **Data Integration**: Combined multiple data sources into unified data model
2. **Data Modeling**: Implemented star schema with proper relationships
3. **KPI Development**: Created calculated measures using DAX expressions
4. **Dashboard Design**: Built interactive visualizations with drill-through capabilities
5. **User Experience**: Designed intuitive interface for business users

### Tools Used
- **Power BI** for dashboard development
- **Power Query** for data transformation
- **DAX** for calculated measures and business logic
- **Star Schema** for data modeling

### Key Lessons Learned
- Business intelligence requires understanding both technical and business requirements
- Proper data modeling is foundational for effective dashboards
- Interactive features enhance user engagement and insight discovery
- Automated reporting significantly reduces manual analysis time

**[View Full Project Details ‚Üí](/projects/hotel-revenue-analytics/)**

---

## üë• Challenge 5: HR Analytics Dashboard Development

### Problem Statement
Design and develop an interactive HR Analytics dashboard to provide comprehensive workforce insights, track key HR metrics, and support data-driven human resources decision making for organizational leadership.

### Approach
1. **Data Preparation**: Imported and transformed HR datasets, ensuring data quality and proper formatting
2. **Metric Development**: Created calculated measures for employee tracking, demographic analysis, and performance indicators
3. **Visualization Design**: Implemented multiple chart types including line charts, bar charts, maps, and advanced visualizations
4. **Dashboard Integration**: Combined visualizations into interactive sections with cross-filtering capabilities
5. **User Experience**: Designed intuitive navigation and interactive features for business users

### Tools Used
- **Tableau** for dashboard development and visualization
- **Data Calculation** functions for custom metrics
- **Interactive Features** including filters, parameters, and tooltips
- **Business Intelligence** principles for KPI design

### Key Lessons Learned
- HR data requires careful handling of demographic and compensation information
- Interactive dashboards significantly enhance user engagement and insight discovery
- Calculated measures enable deeper analysis beyond basic data visualization
- Cross-filtering between visualizations provides comprehensive data exploration
- Business intelligence in HR requires balancing data insights with ethical considerations

**[View Full Project Details ‚Üí](/projects/hr-analytics-dashboard/)**

---

## üè† Challenge 6: House Price Prediction with Linear Regression

### Problem Statement
Develop and evaluate a linear regression model to predict house prices based on property area, implementing a complete machine learning pipeline from data exploration to model deployment and performance assessment.

### Approach
1. **Data Exploration**: Comprehensive analysis of dataset structure, quality, and variable relationships
2. **Visualization**: Implementation of scatter plots, histograms, and correlation analysis
3. **Model Development**: Linear regression training with proper train-test split methodology
4. **Evaluation**: Comprehensive performance metrics including R¬≤ score, MAE, MSE, and RMSE
5. **Validation**: Model application to new data and visualization of predictions

### Tools Used
- **Python** with Scikit-learn for machine learning implementation
- **Pandas & NumPy** for data manipulation and mathematical operations
- **Matplotlib & Seaborn** for statistical visualization
- **Google Colab** for development and execution environment
- **Statistical Analysis** for model interpretation and validation

### Key Lessons Learned
- Linear regression provides interpretable models for continuous prediction tasks
- Proper train-test split is essential for realistic model performance assessment
- Multiple evaluation metrics provide comprehensive model understanding
- Data visualization is crucial for model interpretation and validation
- Feature engineering and dataset size significantly impact model performance

**[View Full Project Details ‚Üí](/projects/house-price-prediction/)**

---

## üç∑ Challenge 7: Multi-Model Wine Classification

### Problem Statement
Implement and compare six different supervised machine learning classification algorithms to predict wine types based on chemical composition features, identifying the most effective model through comprehensive performance evaluation and feature importance analysis.

### Approach
1. **Data Exploration**: Comprehensive analysis of dataset structure, class distribution, and feature correlations
2. **Preprocessing**: Stratified train-test splitting and feature scaling for algorithm compatibility
3. **Model Implementation**: Training of six classification algorithms with appropriate configurations
4. **Evaluation Framework**: Multi-metric assessment including accuracy, precision, recall, F1-score, and confusion matrices
5. **Comparative Analysis**: Performance ranking and feature importance interpretation across all models

### Tools Used
- **Python** with Scikit-learn for machine learning implementation
- **Pandas & NumPy** for data manipulation and analysis
- **Matplotlib & Seaborn** for statistical visualization and comparison
- **Classification Algorithms**: Logistic Regression, Decision Trees, Random Forest, KNN, Naive Bayes, SVM
- **Evaluation Metrics**: Comprehensive performance assessment framework

### Key Lessons Learned
- Multiple algorithms can achieve similar performance on well-structured datasets
- Feature scaling significantly impacts distance-based algorithms like KNN and SVM
- Ensemble methods (Random Forest) often provide robust performance across various scenarios
- Feature importance analysis reveals underlying data patterns and relationships
- Model interpretability varies significantly between algorithms
- Comprehensive evaluation requires multiple metrics beyond simple accuracy

**[View Full Project Details ‚Üí](/projects/wine-classification-ml/)**

---

## ü§ñ Challenge 8: MLOps Pipeline for California Housing Price Prediction

### Problem Statement
Design and implement an end-to-end MLOps pipeline to predict median house prices in California, focusing on reproducibility, scalability, and production readiness while optimizing model performance through systematic hyperparameter tuning and cross-validation.

### Approach
1. **Data Pipeline Construction**: Built integrated preprocessing and modeling pipeline to prevent data leakage
2. **Reproducible Workflows**: Implemented fixed random states and version-controlled configurations
3. **Hyperparameter Optimization**: Systematic GridSearchCV with 5-fold cross-validation over neighbor counts, weighting methods, and distance metrics
4. **Model Validation**: Comprehensive evaluation using R¬≤ score and RMSE with training-test consistency checks
5. **Production Readiness**: Model serialization using pickle for deployment and reuse

### Tools Used
- **Python** with Scikit-learn for complete MLOps implementation
- **Pandas & NumPy** for data manipulation and numerical operations
- **GridSearchCV** for exhaustive hyperparameter optimization with cross-validation
- **Pipeline & ColumnTransformer** for integrated preprocessing and modeling
- **Pickle** for model serialization and persistence
- **Evaluation Metrics**: R¬≤ score, RMSE, cross-validation consistency

### Key Lessons Learned
- MLOps pipelines significantly improve reproducibility and reduce deployment errors
- Integrated preprocessing prevents data leakage and ensures consistent transformations
- Manhattan distance can outperform Euclidean distance for certain feature spaces
- Cross-validation scores close to test scores indicate minimal overfitting
- Model serialization is essential for production deployment and version control
- Pipeline structures enable easy model updates and feature engineering additions
- Systematic hyperparameter search often reveals non-intuitive optimal configurations
- Distance-based weighting improves KNN performance for regression tasks

**[View Full Project Details ‚Üí](/projects/california-housing-mlops/)**

---

## üî¢ Challenge 9: MNIST Digit Classification with Neural Networks

### Problem Statement
Design, implement, and evaluate an Artificial Neural Network (ANN) for handwritten digit recognition using the MNIST dataset, focusing on architectural design, regularization techniques, and comprehensive performance evaluation to achieve production-grade accuracy.

### Approach
1. **Data Processing**: Normalized 70,000 grayscale images and converted pixel values to float format
2. **ANN Architecture**: Designed sequential network with two hidden layers (128‚Üí64 neurons) and strategic dropout
3. **Regularization**: Implemented dropout layers (30%) and early stopping to prevent overfitting
4. **Training Optimization**: Used Adam optimizer with batch processing and validation monitoring
5. **Comprehensive Evaluation**: Multi-metric assessment including accuracy, confusion matrix, and classification reports
6. **Model Persistence**: Saved trained model in Keras format for deployment and reuse

### Tools Used
- **TensorFlow 2.x & Keras** for deep learning implementation
- **NumPy & Pandas** for numerical operations and data handling
- **Matplotlib & Seaborn** for visualization of results and performance metrics
- **Scikit-learn** for evaluation metrics and confusion matrix generation
- **Neural Network Components**: Flatten, Dense, Dropout layers with ReLU/Softmax activations

### Key Lessons Learned
- Data normalization is critical for stable neural network training
- Dropout regularization effectively prevents overfitting in deep networks
- Early stopping based on validation loss optimizes training duration
- Two hidden layers provide sufficient complexity for MNIST classification
- Softmax activation is ideal for multi-class probability distribution
- Adam optimizer balances convergence speed and stability effectively
- Confusion matrices reveal specific digit pairs prone to misclassification
- Model serialization enables practical deployment without retraining
- 97%+ accuracy is achievable with proper architecture and regularization
- Training curves provide valuable insights into model learning behavior

**[View Full Project Details ‚Üí](/projects/mnist-digit-classification-ann/)**

---

## üîß Technical Skills Development

### Data Collection & Preparation
- Web scraping and API integration
- Data cleaning and quality assurance
- Feature engineering and transformation
- Data validation frameworks

### Analysis & Visualization
- Statistical analysis and hypothesis testing
- Exploratory data analysis methodologies
- Business intelligence dashboard development
- Data storytelling and insight communication

### Tools & Technologies Mastered
- **Programming**: Python, DAX
- **Libraries**: Pandas, NumPy, BeautifulSoup, Requests, Matplotlib, Seaborn
- **BI Tools**: Power BI, Power Query
- **Platforms**: Google Colab, Kaggle, Jupyter Notebooks

---

**Explore all my projects in detail: [View Complete Project Portfolio ‚Üí](/projects/)**

*These lab challenges demonstrate my systematic approach to solving data problems and my commitment to continuous skill development in data analytics.*
